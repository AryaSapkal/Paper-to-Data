The aim is to build a tool that can take a scanning of a piece of paper with diagrams, tables, handwritten notes, regular computer-printed text, images, and other information and turn it into digital data that is reformattable and useful.


The plan is to use existing models to perform OCR on computer-printed and handwritten text and train a new segmentation-based model to extract semantic meaning from the relative positions of visual elements of information. These elements could include text paragraphs, charts, graphs, arrows, curly braces, tables, images, icons, and more. 
